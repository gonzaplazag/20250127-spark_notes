{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementación Linear Regression in Pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/10/02 09:28:00 WARN Utils: Your hostname, notebook resolves to a loopback address: 127.0.1.1; using 192.168.0.18 instead (on interface wlp9s0)\n",
      "22/10/02 09:28:00 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/opt/spark-3.0.3/jars/spark-unsafe_2.12-3.0.3.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "22/10/02 09:28:01 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.0.18:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[4]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>linear_reg</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fb4f342dcd0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .master('local[4]')\n",
    "    .appName('linear_reg')\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-----+-----+-----+------+\n",
      "|var_1|var_2|var_3|var_4|var_5|output|\n",
      "+-----+-----+-----+-----+-----+------+\n",
      "|734  |688  |81   |0.328|0.259|0.418 |\n",
      "|700  |600  |94   |0.32 |0.247|0.389 |\n",
      "|712  |705  |93   |0.311|0.247|0.417 |\n",
      "|734  |806  |69   |0.315|0.26 |0.415 |\n",
      "|613  |759  |61   |0.302|0.24 |0.378 |\n",
      "+-----+-----+-----+-----+-----+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load\n",
    "path_input = \"/home/walter/Documents/serie-notas/z_data/20221002_linear_regresion_dataset/data.csv\"\n",
    "\n",
    "data = spark.read.csv(path_input, header = True)\n",
    "\n",
    "data.show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1232 6\n"
     ]
    }
   ],
   "source": [
    "print(data.count(), len(data.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- var_1: integer (nullable = true)\n",
      " |-- var_2: integer (nullable = true)\n",
      " |-- var_3: integer (nullable = true)\n",
      " |-- var_4: double (nullable = true)\n",
      " |-- var_5: double (nullable = true)\n",
      " |-- label: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change types\n",
    "from pyspark.sql.types import IntegerType, DoubleType\n",
    "\n",
    "integer_type = ['var_1', 'var_2', 'var_3']\n",
    "float_type = ['var_4', 'var_5', 'output']\n",
    "\n",
    "for c in integer_type:\n",
    "    data = data.withColumn(c, data[c].cast(IntegerType()))\n",
    "\n",
    "for c in float_type:\n",
    "    data = data.withColumn(c, data[c].cast(DoubleType()))\n",
    "\n",
    "data = data.withColumnRenamed('output', 'label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+-----------------+------------------+--------------------+-------------------+--------------------+\n",
      "|summary|            var_1|            var_2|             var_3|               var_4|              var_5|               label|\n",
      "+-------+-----------------+-----------------+------------------+--------------------+-------------------+--------------------+\n",
      "|  count|             1232|             1232|              1232|                1232|               1232|                1232|\n",
      "|   mean|715.0819805194806|715.0819805194806| 80.90422077922078| 0.32633116848573285|  0.259272727111427| 0.39734172100177056|\n",
      "| stddev| 91.5342940441652|93.07993263118064|11.458139049993724|0.015012772053329796|0.01290722968511499|0.033266899150425924|\n",
      "|    min|              463|              472|                40|  0.2770000100135803|0.21400000154972076|  0.3009999990463257|\n",
      "|    max|             1009|             1103|               116| 0.37299999594688416| 0.2939999997615814|  0.4909999966621399|\n",
      "+-------+-----------------+-----------------+------------------+--------------------+-------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# describe\n",
    "data.describe().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|corr(var_1, label)|\n",
      "+------------------+\n",
      "|0.9187399601657321|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show Correlation\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "data.select(F.corr('var_1', 'label')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prep for Model in pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.linalg import Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------+-------------------+\n",
      "|features                                                 |label              |\n",
      "+---------------------------------------------------------+-------------------+\n",
      "|[734.0,688.0,81.0,0.328000009059906,0.2590000033378601]  |0.4180000126361847 |\n",
      "|[700.0,600.0,94.0,0.3199999928474426,0.24699999392032623]|0.3889999985694885 |\n",
      "|[712.0,705.0,93.0,0.3109999895095825,0.24699999392032623]|0.4169999957084656 |\n",
      "|[734.0,806.0,69.0,0.3149999976158142,0.25999999046325684]|0.41499999165534973|\n",
      "|[613.0,759.0,61.0,0.3019999861717224,0.23999999463558197]|0.3779999911785126 |\n",
      "+---------------------------------------------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# preprocessing-pyspark. build model_df\n",
    "features = ['var_1', 'var_2', 'var_3', 'var_4', 'var_5']\n",
    "\n",
    "vec_assembler = VectorAssembler(inputCols = features, outputCol= 'features')\n",
    "features_df = vec_assembler.transform(data)\n",
    "model_df = features_df.select('features', 'label')\n",
    "model_df.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build train-test\n",
    "train_df, test_df = model_df.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/10/02 09:57:44 WARN Instrumentation: [9c15752e] regParam is zero, which might cause numerical instability and overfitting.\n",
      "22/10/02 09:57:44 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n",
      "22/10/02 09:57:44 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n",
      "22/10/02 09:57:44 WARN LAPACK: Failed to load implementation from: com.github.fommil.netlib.NativeSystemLAPACK\n",
      "22/10/02 09:57:44 WARN LAPACK: Failed to load implementation from: com.github.fommil.netlib.NativeRefLAPACK\n"
     ]
    }
   ],
   "source": [
    "# model\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "lr = LinearRegression(labelCol='label')\n",
    "lr_fitted = lr.fit(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0003369587017994129,5.639903812680925e-05,0.00025517302924750744,-0.7229177737958294,0.5803927116672541]\n"
     ]
    }
   ],
   "source": [
    "# print coef\n",
    "print(lr_fitted.coefficients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1803452510883159\n"
     ]
    }
   ],
   "source": [
    "# print intercept\n",
    "print(lr_fitted.intercept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.876249599311955\n"
     ]
    }
   ],
   "source": [
    "# r2 in training (eval in training)\n",
    "eval_train = lr_fitted.evaluate(train_df)\n",
    "print(eval_train.r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8493379732302191\n"
     ]
    }
   ],
   "source": [
    "# r2 in testing (eval in testing)\n",
    "eval_test = lr_fitted.evaluate(test_df)\n",
    "print(eval_test.r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01258615918477524\n"
     ]
    }
   ],
   "source": [
    "# RMSE in testing (eval in testing)\n",
    "print(eval_test.rootMeanSquaredError)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo anterior muestra los pasos esenciales del proceso. A Continuación lo mismo pero en pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model with Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType, DoubleType\n",
    "\n",
    "# load\n",
    "path_input = \"/home/walter/Documents/serie-notas/z_data/20221002_linear_regresion_dataset/data.csv\"\n",
    "\n",
    "new_df = spark.read.csv(path_input, header = True)\n",
    "\n",
    "# Change types\n",
    "\n",
    "integer_type = ['var_1', 'var_2', 'var_3']\n",
    "float_type = ['var_4', 'var_5', 'output']\n",
    "\n",
    "for c in integer_type:\n",
    "    new_df = new_df.withColumn(c, new_df[c].cast(IntegerType()))\n",
    "\n",
    "for c in float_type:\n",
    "    new_df = new_df.withColumn(c, new_df[c].cast(DoubleType()))\n",
    "\n",
    "new_df = new_df.withColumnRenamed('output', 'label')\n",
    "\n",
    "# splot train test\n",
    "train_df, test_df = new_df.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build pipeline\n",
    "features = ['var_1', 'var_2', 'var_3', 'var_4', 'var_5']\n",
    "\n",
    "stage_1 = VectorAssembler(inputCols=features, outputCol='out_features')\n",
    "stage_2 = StandardScaler(inputCol='out_features', outputCol='features')\n",
    "stage_3 = LinearRegression()\n",
    "stages = [stage_1, stage_2, stage_3]\n",
    "pipeline = Pipeline(stages=stages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/10/02 10:23:50 WARN Instrumentation: [713894c8] regParam is zero, which might cause numerical instability and overfitting.\n"
     ]
    }
   ],
   "source": [
    "# model fit\n",
    "model = pipeline.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-----+-----+-----+-----+------------------------------+----------------------------------------------------------------------------------------------+-------------------+\n",
      "|var_1|var_2|var_3|var_4|var_5|label|out_features                  |features                                                                                      |prediction         |\n",
      "+-----+-----+-----+-----+-----+-----+------------------------------+----------------------------------------------------------------------------------------------+-------------------+\n",
      "|463  |527  |67   |0.284|0.228|0.311|[463.0,527.0,67.0,0.284,0.228]|[4.978771040576806,5.470461860169901,5.77128718287567,18.54647676989447,17.284202823257868]   |0.3116664074117967 |\n",
      "|468  |746  |52   |0.285|0.225|0.329|[468.0,746.0,52.0,0.285,0.225]|[5.032537466500961,7.743765745136141,4.479207962828879,18.61178126556311,17.05677910189921]   |0.31972134553581627|\n",
      "|498  |672  |61   |0.288|0.238|0.325|[498.0,672.0,61.0,0.288,0.238]|[5.355136022045895,6.975617400444352,5.254455494856953,18.807694752569038,18.042281894453385] |0.3323526040091788 |\n",
      "|510  |588  |72   |0.298|0.231|0.317|[510.0,588.0,72.0,0.298,0.231]|[5.484175444263868,6.103665225388808,6.2019802562246005,19.460739709255463,17.511626544616522]|0.3241458076670769 |\n",
      "|511  |576  |76   |0.29 |0.231|0.329|[511.0,576.0,76.0,0.29,0.231] |[5.494928729448699,5.979100628952302,6.5465347149037445,18.938303743906324,17.511626544616522]|0.3297229945922191 |\n",
      "+-----+-----+-----+-----+-----+-----+------------------------------+----------------------------------------------------------------------------------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# predict (test)\n",
    "pred_result = model.transform(test_df)\n",
    "pred_result.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8605285911118769\n"
     ]
    }
   ],
   "source": [
    "# evaluate (test)\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "reg_eval = RegressionEvaluator(labelCol='label', predictionCol='prediction', metricName='rmse')\n",
    "\n",
    "# r2 (eval)\n",
    "acc = reg_eval.evaluate(pred_result, {reg_eval.metricName: 'r2'})\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.012031547226028371\n"
     ]
    }
   ],
   "source": [
    "# rmse (eval)\n",
    "rmse = reg_eval.evaluate(pred_result)\n",
    "print(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('pyspark_1')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d71b34f375d735c8be32cbc17abc1cbe1b4bf3a9e7c70028bca9490fccc04562"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
