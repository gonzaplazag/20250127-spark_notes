{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* main doc: https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.feature.HashingTF.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://WKWZTW3TDtg8tR3.global.publicisgroupe.net:4044\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[4]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>example</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x2253afc6810>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init(\"/spark\")\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .master('local[4]')\n",
    "    .appName('example')\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------+-----+\n",
      "| id|            text|label|\n",
      "+---+----------------+-----+\n",
      "|  0|               b|  1.0|\n",
      "|  0|          hadoop|  1.0|\n",
      "|  0|     spark spark|  1.0|\n",
      "|  0|               b|  1.0|\n",
      "|  0|           b b b|  1.0|\n",
      "|  0|         a spark|  1.0|\n",
      "|  1|             b d|  0.0|\n",
      "|  2|       spark a e|  1.0|\n",
      "|  3|hadoop mapreduce|  0.0|\n",
      "+---+----------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Prepare training documents from a list of (id, text, label) tuples.\n",
    "training = spark.createDataFrame([\n",
    "    (0, \"b\", 1.0),\n",
    "    (0, \"hadoop\", 1.0),\n",
    "    (0, \"spark spark\", 1.0),\n",
    "    (0, \"b\", 1.0),\n",
    "    (0, \"b b b\", 1.0),\n",
    "    (0, \"a spark\", 1.0),\n",
    "    (1, \"b d\", 0.0),\n",
    "    (2, \"spark a e\", 1.0),\n",
    "    (3, \"hadoop mapreduce\", 0.0)\n",
    "], [\"id\", \"text\", \"label\"])\n",
    "\n",
    "training.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------+-----+-------------------+\n",
      "| id|            text|label|              words|\n",
      "+---+----------------+-----+-------------------+\n",
      "|  0|               b|  1.0|                [b]|\n",
      "|  0|          hadoop|  1.0|           [hadoop]|\n",
      "|  0|     spark spark|  1.0|     [spark, spark]|\n",
      "|  0|               b|  1.0|                [b]|\n",
      "|  0|           b b b|  1.0|          [b, b, b]|\n",
      "|  0|         a spark|  1.0|         [a, spark]|\n",
      "|  1|             b d|  0.0|             [b, d]|\n",
      "|  2|       spark a e|  1.0|      [spark, a, e]|\n",
      "|  3|hadoop mapreduce|  0.0|[hadoop, mapreduce]|\n",
      "+---+----------------+-----+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Tokenizer\n",
    "\n",
    "# define tokenizer\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "\n",
    "# transform\n",
    "tokenized = tokenizer.transform(training)\n",
    "tokenized.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0--------------------------------------\n",
      " id       | 0                                  \n",
      " text     | b                                  \n",
      " label    | 1.0                                \n",
      " words    | [b]                                \n",
      " features | (1000,[165],[1.0])                 \n",
      "-RECORD 1--------------------------------------\n",
      " id       | 0                                  \n",
      " text     | hadoop                             \n",
      " label    | 1.0                                \n",
      " words    | [hadoop]                           \n",
      " features | (1000,[585],[1.0])                 \n",
      "-RECORD 2--------------------------------------\n",
      " id       | 0                                  \n",
      " text     | spark spark                        \n",
      " label    | 1.0                                \n",
      " words    | [spark, spark]                     \n",
      " features | (1000,[286],[2.0])                 \n",
      "-RECORD 3--------------------------------------\n",
      " id       | 0                                  \n",
      " text     | b                                  \n",
      " label    | 1.0                                \n",
      " words    | [b]                                \n",
      " features | (1000,[165],[1.0])                 \n",
      "-RECORD 4--------------------------------------\n",
      " id       | 0                                  \n",
      " text     | b b b                              \n",
      " label    | 1.0                                \n",
      " words    | [b, b, b]                          \n",
      " features | (1000,[165],[3.0])                 \n",
      "-RECORD 5--------------------------------------\n",
      " id       | 0                                  \n",
      " text     | a spark                            \n",
      " label    | 1.0                                \n",
      " words    | [a, spark]                         \n",
      " features | (1000,[286,467],[1.0,1.0])         \n",
      "-RECORD 6--------------------------------------\n",
      " id       | 1                                  \n",
      " text     | b d                                \n",
      " label    | 0.0                                \n",
      " words    | [b, d]                             \n",
      " features | (1000,[165,890],[1.0,1.0])         \n",
      "-RECORD 7--------------------------------------\n",
      " id       | 2                                  \n",
      " text     | spark a e                          \n",
      " label    | 1.0                                \n",
      " words    | [spark, a, e]                      \n",
      " features | (1000,[286,467,550],[1.0,1.0,1.0]) \n",
      "-RECORD 8--------------------------------------\n",
      " id       | 3                                  \n",
      " text     | hadoop mapreduce                   \n",
      " label    | 0.0                                \n",
      " words    | [hadoop, mapreduce]                \n",
      " features | (1000,[585,750],[1.0,1.0])         \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# hashing: Maps a sequence of terms to their term frequencies using the hashing trick. \n",
    "# Currently we use Austin Applebyâ€™s MurmurHash 3 algorithm (MurmurHash3_x86_32) \n",
    "# to calculate the hash code value for the term object. Since a simple modulo \n",
    "# is used to transform the hash function to a column index, it is advisable \n",
    "# to use a power of two as the numFeatures parameter; otherwise \n",
    "# the features will not be mapped evenly to the columns.\n",
    "\n",
    "from pyspark.ml.feature import HashingTF\n",
    "\n",
    "# define hashing\n",
    "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"features\")\n",
    "hashingTF.setNumFeatures(1000)\n",
    "\n",
    "# apply hashing\n",
    "hashed = hashingTF.transform(tokenized)\n",
    "hashed.show(10, False, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
