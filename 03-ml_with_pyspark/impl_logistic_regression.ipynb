{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/10/02 19:11:12 WARN Utils: Your hostname, notebook resolves to a loopback address: 127.0.1.1; using 192.168.0.18 instead (on interface wlp9s0)\n",
      "22/10/02 19:11:12 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/opt/spark-3.0.3/jars/spark-unsafe_2.12-3.0.3.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "22/10/02 19:11:12 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.0.18:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[4]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>logisticReg</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f2e2baf6ac0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark=(\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .master('local[4]')\n",
    "    .appName('logisticReg')\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+--------------+--------+----------------+------+\n",
      "|Country  |Age|Repeat_Visitor|Platform|Web_pages_viewed|Status|\n",
      "+---------+---+--------------+--------+----------------+------+\n",
      "|India    |41 |1             |Yahoo   |21              |1     |\n",
      "|Brazil   |28 |1             |Yahoo   |5               |0     |\n",
      "|Brazil   |40 |0             |Google  |3               |0     |\n",
      "|Indonesia|31 |1             |Bing    |15              |1     |\n",
      "|Malaysia |32 |0             |Google  |15              |1     |\n",
      "+---------+---+--------------+--------+----------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "path_input = \"/home/walter/Documents/serie-notas/z_data/20221002_logistic_reg_dataset/data.csv\"\n",
    "\n",
    "data=spark.read.csv(path_input, header=True)\n",
    "\n",
    "data.show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000 6\n"
     ]
    }
   ],
   "source": [
    "print(data.count(), len(data.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Country: string (nullable = true)\n",
      " |-- Age: string (nullable = true)\n",
      " |-- Repeat_Visitor: string (nullable = true)\n",
      " |-- Platform: string (nullable = true)\n",
      " |-- Web_pages_viewed: string (nullable = true)\n",
      " |-- Status: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "integer_type = ['Age', 'Repeat_Visitor', 'Web_pages_viewed', 'Status']\n",
    "\n",
    "for c in integer_type:\n",
    "    data = data.withColumn(c, data[c].cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Country: string (nullable = true)\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- Repeat_Visitor: integer (nullable = true)\n",
      " |-- Platform: string (nullable = true)\n",
      " |-- Web_pages_viewed: integer (nullable = true)\n",
      " |-- Status: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------------+-----------------+------------------+\n",
      "|              Age|   Repeat_Visitor| Web_pages_viewed|            Status|\n",
      "+-----------------+-----------------+-----------------+------------------+\n",
      "|            20000|            20000|            20000|             20000|\n",
      "|         28.53955|           0.5029|           9.5533|               0.5|\n",
      "|7.888912950773227|0.500004090187782|6.073903499824976|0.5000125004687693|\n",
      "|               17|                0|                1|                 0|\n",
      "|              111|                1|               29|                 1|\n",
      "+-----------------+-----------------+-----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.describe().select(integer_type).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n",
      "|  Country|count|\n",
      "+---------+-----+\n",
      "| Malaysia| 1218|\n",
      "|    India| 4018|\n",
      "|Indonesia|12178|\n",
      "|   Brazil| 2586|\n",
      "+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "    data\n",
    "    .groupBy('Country')\n",
    "    .count()\n",
    "    .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|Platform|count|\n",
      "+--------+-----+\n",
      "|   Yahoo| 9859|\n",
      "|    Bing| 4360|\n",
      "|  Google| 5781|\n",
      "+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.groupBy('Platform').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|Status|count|\n",
      "+------+-----+\n",
      "|     1|10000|\n",
      "|     0|10000|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.groupBy('Status').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------------+-------------------+---------------------+--------------------+\n",
      "|  Country|          avg(Age)|avg(Repeat_Visitor)|avg(Web_pages_viewed)|         avg(Status)|\n",
      "+---------+------------------+-------------------+---------------------+--------------------+\n",
      "| Malaysia|27.792282430213465| 0.5730706075533661|   11.192118226600986|  0.6568144499178982|\n",
      "|    India|27.976854156296664| 0.5433051269288203|   10.727227476356397|  0.6212045793927327|\n",
      "|Indonesia| 28.43159796354081| 0.5207751683363442|    9.985711939563148|  0.5422893742814913|\n",
      "|   Brazil|30.274168600154677|  0.322892498066512|    4.921113689095128|0.038669760247486466|\n",
      "+---------+------------------+-------------------+---------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.groupBy('Country').mean().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prep for model in pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+--------------+--------+----------------+------+\n",
      "|Country  |Age|Repeat_Visitor|Platform|Web_pages_viewed|Status|\n",
      "+---------+---+--------------+--------+----------------+------+\n",
      "|India    |41 |1             |Yahoo   |21              |1     |\n",
      "|Brazil   |28 |1             |Yahoo   |5               |0     |\n",
      "|Brazil   |40 |0             |Google  |3               |0     |\n",
      "|Indonesia|31 |1             |Bing    |15              |1     |\n",
      "|Malaysia |32 |0             |Google  |15              |1     |\n",
      "+---------+---+--------------+--------+----------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+--------------+--------+----------------+------+--------------+-------------+\n",
      "|Country  |Age|Repeat_Visitor|Platform|Web_pages_viewed|Status|Platform_index|Country_index|\n",
      "+---------+---+--------------+--------+----------------+------+--------------+-------------+\n",
      "|India    |41 |1             |Yahoo   |21              |1     |0.0           |1.0          |\n",
      "|Brazil   |28 |1             |Yahoo   |5               |0     |0.0           |2.0          |\n",
      "|Brazil   |40 |0             |Google  |3               |0     |1.0           |2.0          |\n",
      "|Indonesia|31 |1             |Bing    |15              |1     |2.0           |0.0          |\n",
      "|Malaysia |32 |0             |Google  |15              |1     |1.0           |3.0          |\n",
      "+---------+---+--------------+--------+----------------+------+--------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# encoding. step 1. Uso de StringIndexer para recodificar var categóricas\n",
    "# por default, el valor de la variable se da en función del count de la categoría\n",
    "\n",
    "si_platform = StringIndexer(inputCol='Platform', outputCol='Platform_index')\n",
    "si_country = StringIndexer(inputCol='Country', outputCol='Country_index')\n",
    "data = si_platform.fit(data).transform(data)\n",
    "data = si_country.fit(data).transform(data)\n",
    "data.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+--------------+--------+----------------+------+--------------+-------------+-------------+-------------+\n",
      "|Country  |Age|Repeat_Visitor|Platform|Web_pages_viewed|Status|Platform_index|Country_index|Platform_vec |Country_vec  |\n",
      "+---------+---+--------------+--------+----------------+------+--------------+-------------+-------------+-------------+\n",
      "|India    |41 |1             |Yahoo   |21              |1     |0.0           |1.0          |(2,[0],[1.0])|(3,[1],[1.0])|\n",
      "|Brazil   |28 |1             |Yahoo   |5               |0     |0.0           |2.0          |(2,[0],[1.0])|(3,[2],[1.0])|\n",
      "|Brazil   |40 |0             |Google  |3               |0     |1.0           |2.0          |(2,[1],[1.0])|(3,[2],[1.0])|\n",
      "|Indonesia|31 |1             |Bing    |15              |1     |2.0           |0.0          |(2,[],[])    |(3,[0],[1.0])|\n",
      "|Malaysia |32 |0             |Google  |15              |1     |1.0           |3.0          |(2,[1],[1.0])|(3,[],[])    |\n",
      "+---------+---+--------------+--------+----------------+------+--------------+-------------+-------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# encoding. step 2. One hot encoding\n",
    "\n",
    "from pyspark.ml.feature import OneHotEncoder\n",
    "\n",
    "encoder = OneHotEncoder(inputCols=['Platform_index', 'Country_index'], outputCols=['Platform_vec', 'Country_vec'])\n",
    "\n",
    "data = encoder.fit(data).transform(data)\n",
    "\n",
    "\n",
    "data.show(5, False)\n",
    "\n",
    "# la interpretación del OnehotEncoder es la siguiente: (3,[1],[1.0]) : un vector de len 3, en la posición 1, valor 1.0\n",
    "\n",
    "# from Pramod (2022), pg. 94:\n",
    "#This kind of representation allows to save computational space and hence is faster to\n",
    "#compute. The length of the vector is equal to one less than the total number of elements\n",
    "#since each value can be easily represented with just the help of three columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------+-----+\n",
      "|features                       |label|\n",
      "+-------------------------------+-----+\n",
      "|[41.0,21.0,1.0,0.0,0.0,1.0,0.0]|1    |\n",
      "|[28.0,5.0,1.0,0.0,0.0,0.0,1.0] |0    |\n",
      "|[40.0,3.0,0.0,1.0,0.0,0.0,1.0] |0    |\n",
      "|(7,[0,1,4],[31.0,15.0,1.0])    |1    |\n",
      "|(7,[0,1,3],[32.0,15.0,1.0])    |1    |\n",
      "+-------------------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Vector Assembler\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "vec_assembler = VectorAssembler(inputCols=['Age', 'Web_pages_viewed', 'Platform_vec', 'Country_vec'], outputCol='features')\n",
    "features_df = vec_assembler.transform(data)\n",
    "model_df = features_df.select('features', 'Status')\n",
    "model_df = model_df.withColumnRenamed('Status', 'label')\n",
    "model_df.show(5, False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split train test\n",
    "train_df, test_df = model_df.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|label|count|\n",
      "+-----+-----+\n",
      "|    1| 6960|\n",
      "|    0| 6998|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check class balance in train test\n",
    "train_df.groupBy('label').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|label|count|\n",
      "+-----+-----+\n",
      "|    1| 3040|\n",
      "|    0| 3002|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_df.groupBy('label').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/10/02 19:11:25 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n",
      "22/10/02 19:11:25 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "log_reg = LogisticRegression(labelCol='label').fit(train_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+-----+----------------------------------------+----------------------------------------+----------+\n",
      "|features             |label|rawPrediction                           |probability                             |prediction|\n",
      "+---------------------+-----+----------------------------------------+----------------------------------------+----------+\n",
      "|(7,[0,1],[17.0,3.0]) |0    |[3.2825143224442486,-3.2825143224442486]|[0.9638240534393888,0.03617594656061128]|0.0       |\n",
      "|(7,[0,1],[17.0,3.0]) |0    |[3.2825143224442486,-3.2825143224442486]|[0.9638240534393888,0.03617594656061128]|0.0       |\n",
      "|(7,[0,1],[17.0,4.0]) |0    |[2.540997224459203,-2.540997224459203]  |[0.926966367147523,0.07303363285247705] |0.0       |\n",
      "|(7,[0,1],[17.0,8.0]) |1    |[-0.4250711674809793,0.4250711674809793]|[0.3953039033751607,0.6046960966248394] |1.0       |\n",
      "|(7,[0,1],[17.0,10.0])|1    |[-1.9081053634510692,1.9081053634510692]|[0.12919385436053135,0.8708061456394686]|1.0       |\n",
      "+---------------------+-----+----------------------------------------+----------------------------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# function evaluate(): agrega probability (vector neg_class / pos_class) y prediction (thr = 0.5)\n",
    "\n",
    "train_results = log_reg.evaluate(train_df).predictions\n",
    "train_results.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test data\n",
    "test_results = log_reg.evaluate(test_df).predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics (results):\n",
    "    true_positives = results[(results.label == 1) & (results.prediction== 1)].count()\n",
    "    true_negatives = results[(results.label == 0) & (results.prediction== 0)].count()\n",
    "    false_positives = results[(results.label == 0) & (results.prediction== 1)].count()\n",
    "    false_negatives = results[(results.label == 1) & (results.prediction== 0)].count()\n",
    "\n",
    "    accuracy=float((true_positives+true_negatives) /(results.count()))\n",
    "    recall = float(true_positives)/(true_positives + false_negatives)\n",
    "    precision = float(true_positives) / (true_positives + false_positives)\n",
    "\n",
    "    print('accuracy: {}, precision: {}, recall: {}'.format(accuracy, precision, recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.9339622641509434, precision: 0.9371069182389937, recall: 0.93125\n"
     ]
    }
   ],
   "source": [
    "metrics(test_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model with Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import OneHotEncoder\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# paths\n",
    "path_input = \"/home/walter/Documents/serie-notas/z_data/20221002_logistic_reg_dataset/data.csv\"\n",
    "\n",
    "# params\n",
    "integer_type = ['Age', 'Repeat_Visitor', 'Web_pages_viewed', 'Status']\n",
    "\n",
    "# functions\n",
    "def change_type(df, integer_type):\n",
    "    res = df\n",
    "    for c in integer_type:\n",
    "        res = res.withColumn(c, res[c].cast(IntegerType()))\n",
    "\n",
    "    return res\n",
    "\n",
    "def metrics (results):\n",
    "    true_positives = results[(results.label == 1) & (results.prediction== 1)].count()\n",
    "    true_negatives = results[(results.label == 0) & (results.prediction== 0)].count()\n",
    "    false_positives = results[(results.label == 0) & (results.prediction== 1)].count()\n",
    "    false_negatives = results[(results.label == 1) & (results.prediction== 0)].count()\n",
    "\n",
    "    accuracy=float((true_positives+true_negatives) /(results.count()))\n",
    "    recall = float(true_positives)/(true_positives + false_negatives)\n",
    "    precision = float(true_positives) / (true_positives + false_positives)\n",
    "\n",
    "    print('accuracy: {}, precision: {}, recall: {}'.format(accuracy, precision, recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load\n",
    "data = spark.read.csv(path_input, header=True)\n",
    "\n",
    "# split train test\n",
    "train_df, test_df = data.randomSplit([0.7, 0.3])\n",
    "\n",
    "# change types\n",
    "train_df = change_type(train_df, integer_type)\n",
    "test_df = change_type(test_df, integer_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load\n",
    "data = spark.read.csv(path_input, header=True)\n",
    "\n",
    "# split train test\n",
    "train_df, test_df = data.randomSplit([0.7, 0.3])\n",
    "\n",
    "# change types\n",
    "train_df = change_type(train_df, integer_type)\n",
    "test_df = change_type(train_df, integer_type)\n",
    "\n",
    "# Columnrenamed\n",
    "train_df = train_df.withColumnRenamed('Status', 'label')\n",
    "test_df = test_df.withColumnRenamed('Status', 'label')\n",
    "\n",
    "# define pipe\n",
    "stage_1 = StringIndexer(inputCol='Country', outputCol='Country_index')\n",
    "stage_2 = StringIndexer(inputCol='Platform', outputCol='Platform_index')\n",
    "stage_3 = OneHotEncoder(inputCols=['Country_index', 'Platform_index'], outputCols=['Country_vec', 'Platform_vec'])\n",
    "stage_4 = VectorAssembler(inputCols=['Age', 'Repeat_Visitor', 'Web_pages_viewed', 'Platform_vec', 'Country_vec'], outputCol='features')\n",
    "stage_5 = LogisticRegression(featuresCol='features', labelCol='label')\n",
    "\n",
    "pipe = Pipeline(stages=[stage_1, stage_2, stage_3, stage_4, stage_5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "model = pipe.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.9409793998146696, precision: 0.9422106931834558, recall: 0.9386934673366835\n"
     ]
    }
   ],
   "source": [
    "# test data\n",
    "test_results = model.transform(test_df)\n",
    "metrics(test_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('pyspark_1')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d71b34f375d735c8be32cbc17abc1cbe1b4bf3a9e7c70028bca9490fccc04562"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
